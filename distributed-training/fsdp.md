* https://huggingface.co/docs/transformers/model_doc/t5



## 참고자료 ##

* [pytorch FSDP](https://pytorch.org/tutorials/intermediate/FSDP_tutorial.html?utm_source=distr_landing&utm_medium=FSDP_getting_started)
* [pytorch FSDP advanced](https://tutorials.pytorch.kr/intermediate/FSDP_adavnced_tutorial.html)
* [Writing Distributed Applications with PyTorch](https://pytorch.org/tutorials/intermediate/dist_tuto.html)
* https://velog.io/@mmsori/PyTorch-Distributed-Sampler-in-evaluation
* https://pytorch.org/blog/introducing-pytorch-fully-sharded-data-parallel-api/
* [다중 GPU를 효율적으로 사용하는 방법: DP부터 FSDP까지](https://medium.com/tesser-team/%EB%8B%A4%EC%A4%91-gpu%EB%A5%BC-%ED%9A%A8%EC%9C%A8%EC%A0%81%EC%9C%BC%EB%A1%9C-%EC%82%AC%EC%9A%A9%ED%95%98%EB%8A%94-%EB%B0%A9%EB%B2%95-dp%EB%B6%80%ED%84%B0-fsdp%EA%B9%8C%EC%A7%80-3057d31150b6)
* https://runebook.dev/ko/docs/pytorch/distributed#google_vignette
* [다수의 GPU 중 원하는 GPU 타겟팅하기](https://wonder-j.tistory.com/15)
* [processing time 정확히 측정하기 - torch.cuda.synchronize()](https://kalelpark.tistory.com/136)
* [Adatdelta Optimizer](https://wikidocs.net/157281)
* [러닝 레이트 스케쥴러](https://gaussian37.github.io/dl-pytorch-lr_scheduler/)
* [What is the difference between rank and local-rank?](https://discuss.pytorch.org/t/what-is-the-difference-between-rank-and-local-rank/61940)
* https://kongsberg.tistory.com/7#google_vignette
## python ##

* [Functools 의 Partial 이란?](https://hamait.tistory.com/823)
