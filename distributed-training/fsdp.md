



## 참고자료 ##

* [pytorch FSDP](https://pytorch.org/tutorials/intermediate/FSDP_tutorial.html?utm_source=distr_landing&utm_medium=FSDP_getting_started)
* [Writing Distributed Applications with PyTorch](https://pytorch.org/tutorials/intermediate/dist_tuto.html)
* [다중 GPU를 효율적으로 사용하는 방법: DP부터 FSDP까지](https://medium.com/tesser-team/%EB%8B%A4%EC%A4%91-gpu%EB%A5%BC-%ED%9A%A8%EC%9C%A8%EC%A0%81%EC%9C%BC%EB%A1%9C-%EC%82%AC%EC%9A%A9%ED%95%98%EB%8A%94-%EB%B0%A9%EB%B2%95-dp%EB%B6%80%ED%84%B0-fsdp%EA%B9%8C%EC%A7%80-3057d31150b6)
* [다수의 GPU 중 원하는 GPU 타겟팅하기](https://wonder-j.tistory.com/15)
* [processing time 정확히 측정하기 - torch.cuda.synchronize()](https://kalelpark.tistory.com/136)
* [Adatdelta Optimizer](https://wikidocs.net/157281)
* [러닝 레이트 스케쥴러](https://gaussian37.github.io/dl-pytorch-lr_scheduler/)
